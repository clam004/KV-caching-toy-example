{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ba90b98-bb13-44b4-b609-b8095794f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "    \n",
    "from pico.utils import load_encoder_hparams_and_params\n",
    "from pico.gpt2 import generate, gelu, linear, softmax, layer_norm, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641f966-b125-4e95-bf09-3b21b940f766",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "1. https://www.dipkumar.dev/posts/gpt-kvcache/\n",
    "2. https://github.com/jaymody/picoGPT/pull/7/files\n",
    "\n",
    "You can also control the number of tokens to generate, the model size (one of `[\"124M\", \"355M\", \"774M\", \"1558M\"]`), and the directory to save the models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24853542-495d-4ab9-b6c9-76d7575ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kb [00:00, 1.76Mb/s]                                                       \n",
      "Fetching encoder.json: 1.04Mb [00:00, 2.48Mb/s]                                                     \n",
      "Fetching hparams.json: 1.00kb [00:00, 2.67Mb/s]                                                     \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mb [01:24, 5.91Mb/s]                                    \n",
      "Fetching model.ckpt.index: 6.00kb [00:00, 6.91Mb/s]                                                 \n",
      "Fetching model.ckpt.meta: 472kb [00:00, 1.73Mb/s]                                                   \n",
      "Fetching vocab.bpe: 457kb [00:00, 1.75Mb/s]                                                         \n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_generate = 40\n",
    "tokenizer, hparams, params = load_encoder_hparams_and_params(model_size = \"124M\", models_dir = \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2b01d21b-4d4b-4f9f-ace7-a85b2f13b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "# make sure we are not surpassing the max sequence length of our model\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "expected_completion = ' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5841c5a2-2733-4d25-9dd1-33c41edb3800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  10%|████████████████▊                                                                                                                                                       | 4/40 [00:00<00:01, 19.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (10, 10)\n",
      "QK_T.shape (11, 11)\n",
      "QK_T.shape (12, 12)\n",
      "QK_T.shape (13, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  15%|█████████████████████████▏                                                                                                                                              | 6/40 [00:00<00:01, 18.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (14, 14)\n",
      "QK_T.shape (15, 15)\n",
      "QK_T.shape (16, 16)\n",
      "QK_T.shape (17, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  25%|█████████████████████████████████████████▊                                                                                                                             | 10/40 [00:00<00:01, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (18, 18)\n",
      "QK_T.shape (19, 19)\n",
      "QK_T.shape (20, 20)\n",
      "QK_T.shape (21, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  35%|██████████████████████████████████████████████████████████▍                                                                                                            | 14/40 [00:00<00:01, 14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (22, 22)\n",
      "QK_T.shape (23, 23)\n",
      "QK_T.shape (24, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  40%|██████████████████████████████████████████████████████████████████▊                                                                                                    | 16/40 [00:01<00:01, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (25, 25)\n",
      "QK_T.shape (26, 26)\n",
      "QK_T.shape (27, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  50%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                   | 20/40 [00:01<00:01, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (28, 28)\n",
      "QK_T.shape (29, 29)\n",
      "QK_T.shape (30, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  55%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                           | 22/40 [00:01<00:01,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (31, 31)\n",
      "QK_T.shape (32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 24/40 [00:01<00:01,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (33, 33)\n",
      "QK_T.shape (34, 34)\n",
      "QK_T.shape (35, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                          | 26/40 [00:02<00:01,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (36, 36)\n",
      "QK_T.shape (37, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 29/40 [00:02<00:01,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (38, 38)\n",
      "QK_T.shape (39, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                     | 31/40 [00:02<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (40, 40)\n",
      "QK_T.shape (41, 41)\n",
      "QK_T.shape (42, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 34/40 [00:03<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (43, 43)\n",
      "QK_T.shape (44, 44)\n",
      "QK_T.shape (45, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 37/40 [00:03<00:00,  8.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (46, 46)\n",
      "QK_T.shape (47, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK_T.shape (48, 48)\n",
      "QK_T.shape (49, 49)\n",
      "The process took 3.7076900005340576 seconds to complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "# decode the ids back into a string\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The process took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d7979-9cba-4ad0-bf1e-db7e6ea45040",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Before we learn kv-cache, lets first understand the non-kv-cache version of the autoregressive generation.\n",
    "\n",
    "First, a tokenizer converts our text into a list of token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "251ca01a-7357-4c5b-808f-f8748955f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input string using the BytePairEncoding tokenizer\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "print(len(input_ids))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc3434-1abd-46b4-9f60-4d992c4f7303",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "The weights to this LLM are the\n",
    "\n",
    "1. Word Positional Encoding (wpe)\n",
    "2. Word Token Embeddings (wte)\n",
    "\n",
    "```python\n",
    "print(params.keys()) # dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])\n",
    "```\n",
    "\n",
    "By passing `**params` into `logits = gpt2(inputs, **params, n_head=n_head)` we are just passing this dictionary's values, the weights akak parameters, into the function as arguments using the dictionary keys as the names `wte, wpe, blocks, ln_f`\n",
    "\n",
    "#### Word Positional Encoding\n",
    "\n",
    "The Word Positional Encoding (wpe) is used to add a vector that represents a position in time, or order in a sequence, to each token embedding. its `print(type(params['wpe']), params['wpe'].shape)` is `<class 'numpy.ndarray'> (1024, 768)` because we have precalculated for you the first 1024 of these positional embeddings, and our embedding size is 768. In doing `wpe[range(len(inputs))]` we have just selected the first `len(inputs)` embeddings\n",
    "\n",
    "#### Word Token Embedding\n",
    "\n",
    "The Word Token Embedding is used to map each token_id (input_ids) to its corresponding vector. `print(type(params['wte']), params['wte'].shape)` is `<class 'numpy.ndarray'> (50257, 768)` because our vocab size is 50257 and our embedding size is 768. In doing `wte[inputs]` we have just mapped our token id list of size 10 to a sequence of embeddings shape (10, 768)\n",
    "\n",
    "#### Transformer Input Embeddings\n",
    "\n",
    " the embedings that go into the first of multiple transformer blocks is the element-wise sum of wte and wpe `x = wte[inputs] + wpe[range(len(inputs))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff499b14-d0bb-4420-9270-23d21fe42b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.88207198e-02 -1.97418600e-01  4.02672496e-03 ... -4.30437364e-02\n",
      "   2.82671917e-02  5.44901080e-02]\n",
      " [ 2.39594337e-02 -5.37920333e-02 -9.48786438e-02 ...  3.41700129e-02\n",
      "   1.01718502e-02 -1.55729489e-04]\n",
      " [ 4.21607168e-03 -8.47639143e-02  5.45149297e-02 ...  1.97447110e-02\n",
      "   1.93248559e-02 -2.14238558e-02]\n",
      " ...\n",
      " [ 2.53077131e-03 -3.17870919e-03  1.17414258e-01 ...  2.00962462e-03\n",
      "   4.41795774e-03 -6.83258474e-03]\n",
      " [-1.23805739e-03 -1.77337788e-03  1.11044556e-01 ... -2.30074697e-03\n",
      "   4.15364839e-03 -1.04475096e-02]\n",
      " [ 4.93714586e-03  2.14576256e-03  1.17781341e-01 ... -2.82027118e-04\n",
      "   4.07085707e-03 -5.54985739e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wpe'][range(len(input_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "882d9fd3-ac2c-4b74-8340-9e6828486d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04486499 -0.1522257   0.10908855 ...  0.16187134  0.00406003\n",
      "  -0.01259668]\n",
      " [-0.1435177  -0.1303647  -0.00709237 ... -0.26905674 -0.21710931\n",
      "  -0.27703205]\n",
      " [-0.14161602 -0.06058507  0.05428597 ...  0.16568261  0.1750053\n",
      "   0.08499283]\n",
      " ...\n",
      " [ 0.00818344  0.03351058  0.03436588 ...  0.15731247  0.06635052\n",
      "  -0.08678364]\n",
      " [-0.1378994  -0.02936367 -0.00255402 ... -0.09662744 -0.07259481\n",
      "   0.11599892]\n",
      " [ 0.06102467 -0.072351    0.01882253 ... -0.24272189  0.23248099\n",
      "   0.12684126]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wte'][input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2a36077-cd72-4c82-9365-58b6759ba78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = params['wte']\n",
    "wpe = params['wpe']\n",
    "x = wte[input_ids] + wpe[range(len(input_ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e7855-bbe5-4e6b-9284-4c12b76c57a2",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "The blocks are a list of repeating transformer blocks `type(params['blocks']) # list` where each block `params['blocks'][0].keys()` consists of ` dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])`.\n",
    "\n",
    "```python\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "#### layer norm\n",
    "\n",
    "The layer_norm weights `params['blocks'][0]['ln_1'].keys()` consist of a gamma and beta params`dict_keys(['b', 'g'])` which are also called the scale and offset weights because g multiples each element by a factor and be shifts the entire vector `g * x + b` , both  `g` and `b` have the same shape `(768,)`\n",
    "\n",
    "#### multi-layer-perceptron (mlp) aka feed forward net (ffn) \n",
    "\n",
    "This is covered in most basic machine learning classes, so it should suffice that in NumPy, the `@` symbol is used as the matrix multiplication operator, that `ffn` has the same input and output shape and that this is the implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "    return x\n",
    "```\n",
    "\n",
    "Both the layer norm, the ffn and multi headed attention (mha) and the overall transformer block have the same input and output shape\n",
    "\n",
    "#### causal mask\n",
    "\n",
    "```python\n",
    "# causal mask to hide future inputs from being attended to\n",
    "# [n_seq, n_seq]\n",
    "causal_mask = (1 - np.tri(3, dtype=x.dtype)) * -1e10  \n",
    "causal_mask\n",
    "```\n",
    "\n",
    "```\n",
    "array([[-0.e+00, -1.e+10, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -0.e+00]], dtype=float32)\n",
    "```\n",
    "\n",
    "The very negative values cause these positions to have an attention score of nearly 0 after the row-wise softmax is applied.\n",
    "Causing no attention weight to be placed on future tokens\n",
    "\n",
    "```\n",
    "[[ 0, -1000,   -1000],\n",
    " [ 0,     0,   -1000],\n",
    " [ 0,     0,       0]]\n",
    "```\n",
    "#### Attention (Scaled Dot Product QKV attention)\n",
    "\n",
    "Here is a moving diagram of Scaled Dot Product QKV attention as a tranformer starts from 1 token and each of the next 3 tokens it generates attends over the previous positions. The grey squares represent the causal mask and though not shown in the diagram, a softmax is applied to `QK^T` before it is matrix multiplied with V to produce A. Watch the diagram evolve, notice that at each step, the upper left square of the `QK^T` matrix is recomputed at every step. Only the bottom row and right column are new. Not only that, notice that because of causal masking, only the bottom row is both new and also need, for matrix multiplication with V to find the next A vector. kv-caching has to do with improving the efficiency for this attention step. \n",
    "\n",
    "<img src=\"samples/QKV_scaled_dot_prod_attn.gif\" height = 500 width = 1000 >\n",
    "\n",
    "```python\n",
    "# Q, K, V -> A\n",
    "def attention(Q, K, V, mask): \n",
    "    \n",
    "    # [n_seq_q, n_embd], [n_seq_k, n_embd], [n_seq_k, n_embd], [n_seq_q, n_seq_k] -> [n_seq_q, n_embd]\n",
    "    \n",
    "    QK_T = Q @ K.T\n",
    "    \n",
    "    A = softmax(QK_T / np.sqrt(Q.shape[-1]) + mask) @ V\n",
    "    \n",
    "    return A\n",
    "```\n",
    "\n",
    "#### Multi Headed Attention (mha)\n",
    "\n",
    "multi-headed attention is instead of applying the attention function to Q K V, chopping Q, K V\n",
    "into multiple segments and applying attention between those corresponding segments, then concatenating the result\n",
    "\n",
    "```python\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def mha(x, c_attn, c_proj, n_head):  \n",
    "    \n",
    "    # qkv projection\n",
    "    # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "    x = linear(x, **c_attn)  \n",
    "\n",
    "    # split into qkv\n",
    "    # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "    qkv = np.split(x, 3, axis=-1)  \n",
    "\n",
    "    # split into heads\n",
    "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    # [n_seq, n_seq]\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  \n",
    "\n",
    "    # perform attention over each head\n",
    "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  \n",
    "    \n",
    "    # merge heads\n",
    "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "    x = np.hstack(out_heads)  \n",
    "\n",
    "    # out projection\n",
    "    # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    x = linear(x, **c_proj)  \n",
    "\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a114a6b0-21e4-413a-9b6f-a5feb2b02c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv_proj.shape (10, 2304)\n",
      "[ 0.01042262  0.22827548 -0.7129095  -0.9784453 ]\n",
      "len(qkv),(qkv[0].shape) 3 (10, 768)\n",
      "len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape 3 12 (10, 64)\n",
      "[ 0.01042262  0.22827548 -0.7129095  -0.9784453 ]\n",
      "len(out_heads), out_heads[0].shape 12 (10, 64)\n",
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "# code run thru of multi-headed attention\n",
    "\n",
    "ln_1 = params['blocks'][0]['ln_1']\n",
    "attn = params['blocks'][0]['attn']\n",
    "n_head = hparams['n_head']\n",
    "c_attn = attn['c_attn']\n",
    "c_proj = attn['c_proj']\n",
    "\n",
    "x_ln = layer_norm(x, **ln_1) # x thanks been layer normed\n",
    "\n",
    "qkv_proj = linear(x_ln, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "print(\"qkv_proj.shape\",qkv_proj.shape) # (10, 2304), 768 x 3 = 2304\n",
    "print(qkv_proj[0,:4])\n",
    "\n",
    "qkv = np.split(qkv_proj, 3, axis=-1) # [n_seq, 3*n_embd] -> List[3, (n_seq, n_embd)]\n",
    "print(\"len(qkv),(qkv[0].shape)\",len(qkv),(qkv[0].shape)) # list of each head's qkv projection \n",
    "\n",
    "# split into heads\n",
    "# [3, n_seq, n_embd] -> List[3, List[n_head (n_seq, n_embd/n_head)]]\n",
    "qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "print(\"len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape\", len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape)\n",
    "print(qkv_heads[0][0][0,:4])\n",
    "\n",
    "# causal mask to hide future inputs from being attended to\n",
    "causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "# perform attention over each head\n",
    "# List[3, List[n_head (n_seq, n_embd/n_head)]]] -> List[n_head, (n_seq, n_embd/n_head)]\n",
    "out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] \n",
    "\n",
    "print(\"len(out_heads), out_heads[0].shape\", len(out_heads), out_heads[0].shape)\n",
    "\n",
    "# merge heads\n",
    "# List[n_head, (n_seq, n_embd/n_head)] -> [n_seq, n_embd]\n",
    "x_out = np.hstack(out_heads)  # stack horizontally, meaning preserve the\n",
    "\n",
    "# out projection\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "x_out = linear(x_out, **c_proj)  \n",
    "print(x_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7b15c-a7d7-4af2-b75a-73d4cf3e3225",
   "metadata": {},
   "source": [
    "# KV Caching\n",
    "\n",
    "The moving diagram below compares non-kv-caching on top with kv-caching on the bottom row as the transformer starts with 1 token and generate 3 more autoregressively step wise. With each new step, we only calculate the QKV projection for the most recent token. As we discussed previously, only the bottom row of the `QK^T` attention matrix is used at each step. To compute the next bottom row, you dont actually need the previous Q projections, so thats why this isnt called a QKV cache. You do still need all the previous K projections and V projections, also `QK^T` is no longer a square attention matrix but rather a new sequence_length sized vector at each step that represents the newest V projection's attention on all previous V projections.\n",
    "\n",
    "<img src=\"samples/KV_cache.gif\">\n",
    "\n",
    "THe benefit is that instead of doing a (n_seq x emb_dim) x (emb_dim x n_seq) -> O(n_seq^2 x emb_dim) of compute, you now are doing\n",
    "The tradeoff is that we now need to keep a growing Key and Value states in GPU VRAM or CPU RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8574b-ce8d-44cc-9924-50ae61e9da49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9133fd-6834-43fa-bd3e-6600cb48f5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
