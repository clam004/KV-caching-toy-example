{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ba90b98-bb13-44b4-b609-b8095794f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "    \n",
    "from pico.utils import load_encoder_hparams_and_params\n",
    "from pico.gpt2 import generate, gelu, linear, softmax, layer_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641f966-b125-4e95-bf09-3b21b940f766",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "1. https://www.dipkumar.dev/posts/gpt-kvcache/\n",
    "2. https://github.com/jaymody/picoGPT/pull/7/files\n",
    "\n",
    "You can also control the number of tokens to generate, the model size (one of `[\"124M\", \"355M\", \"774M\", \"1558M\"]`), and the directory to save the models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24853542-495d-4ab9-b6c9-76d7575ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kb [00:00, 1.76Mb/s]                                                       \n",
      "Fetching encoder.json: 1.04Mb [00:00, 2.48Mb/s]                                                     \n",
      "Fetching hparams.json: 1.00kb [00:00, 2.67Mb/s]                                                     \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mb [01:24, 5.91Mb/s]                                    \n",
      "Fetching model.ckpt.index: 6.00kb [00:00, 6.91Mb/s]                                                 \n",
      "Fetching model.ckpt.meta: 472kb [00:00, 1.73Mb/s]                                                   \n",
      "Fetching vocab.bpe: 457kb [00:00, 1.75Mb/s]                                                         \n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_generate = 40\n",
    "tokenizer, hparams, params = load_encoder_hparams_and_params(model_size = \"124M\", models_dir = \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b01d21b-4d4b-4f9f-ace7-a85b2f13b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "# make sure we are not surpassing the max sequence length of our model\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "expected_completion = ' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5841c5a2-2733-4d25-9dd1-33c41edb3800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 3.243122100830078 seconds to complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "# decode the ids back into a string\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The process took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d7979-9cba-4ad0-bf1e-db7e6ea45040",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Before we learn kv-cache, lets first understand the non-kv-cache version of the autoregressive generation.\n",
    "\n",
    "First, a tokenizer converts our text into a list of token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "251ca01a-7357-4c5b-808f-f8748955f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input string using the BytePairEncoding tokenizer\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "print(len(input_ids))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc3434-1abd-46b4-9f60-4d992c4f7303",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "The weights to this LLM are the\n",
    "\n",
    "1. Word Positional Encoding (wpe)\n",
    "2. Word Token Embeddings (wte)\n",
    "\n",
    "```python\n",
    "print(params.keys()) # dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])\n",
    "```\n",
    "\n",
    "By passing `**params` into `logits = gpt2(inputs, **params, n_head=n_head)` we are just passing this dictionary's values, the weights akak parameters, into the function as arguments using the dictionary keys as the names `wte, wpe, blocks, ln_f`\n",
    "\n",
    "#### Word Positional Encoding\n",
    "\n",
    "The Word Positional Encoding (wpe) is used to add a vector that represents a position in time, or order in a sequence, to each token embedding. its `print(type(params['wpe']), params['wpe'].shape)` is `<class 'numpy.ndarray'> (1024, 768)` because we have precalculated for you the first 1024 of these positional embeddings, and our embedding size is 768. In doing `wpe[range(len(inputs))]` we have just selected the first `len(inputs)` embeddings\n",
    "\n",
    "#### Word Token Embedding\n",
    "\n",
    "The Word Token Embedding is used to map each token_id (input_ids) to its corresponding vector. `print(type(params['wte']), params['wte'].shape)` is `<class 'numpy.ndarray'> (50257, 768)` because our vocab size is 50257 and our embedding size is 768. In doing `wte[inputs]` we have just mapped our token id list of size 10 to a sequence of embeddings shape (10, 768)\n",
    "\n",
    "#### Transformer Input Embeddings\n",
    "\n",
    " the embedings that go into the first of multiple transformer blocks is the element-wise sum of wte and wpe `x = wte[inputs] + wpe[range(len(inputs))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff499b14-d0bb-4420-9270-23d21fe42b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.88207198e-02 -1.97418600e-01  4.02672496e-03 ... -4.30437364e-02\n",
      "   2.82671917e-02  5.44901080e-02]\n",
      " [ 2.39594337e-02 -5.37920333e-02 -9.48786438e-02 ...  3.41700129e-02\n",
      "   1.01718502e-02 -1.55729489e-04]\n",
      " [ 4.21607168e-03 -8.47639143e-02  5.45149297e-02 ...  1.97447110e-02\n",
      "   1.93248559e-02 -2.14238558e-02]\n",
      " ...\n",
      " [ 2.53077131e-03 -3.17870919e-03  1.17414258e-01 ...  2.00962462e-03\n",
      "   4.41795774e-03 -6.83258474e-03]\n",
      " [-1.23805739e-03 -1.77337788e-03  1.11044556e-01 ... -2.30074697e-03\n",
      "   4.15364839e-03 -1.04475096e-02]\n",
      " [ 4.93714586e-03  2.14576256e-03  1.17781341e-01 ... -2.82027118e-04\n",
      "   4.07085707e-03 -5.54985739e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wpe'][range(len(input_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "882d9fd3-ac2c-4b74-8340-9e6828486d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04486499 -0.1522257   0.10908855 ...  0.16187134  0.00406003\n",
      "  -0.01259668]\n",
      " [-0.1435177  -0.1303647  -0.00709237 ... -0.26905674 -0.21710931\n",
      "  -0.27703205]\n",
      " [-0.14161602 -0.06058507  0.05428597 ...  0.16568261  0.1750053\n",
      "   0.08499283]\n",
      " ...\n",
      " [ 0.00818344  0.03351058  0.03436588 ...  0.15731247  0.06635052\n",
      "  -0.08678364]\n",
      " [-0.1378994  -0.02936367 -0.00255402 ... -0.09662744 -0.07259481\n",
      "   0.11599892]\n",
      " [ 0.06102467 -0.072351    0.01882253 ... -0.24272189  0.23248099\n",
      "   0.12684126]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wte'][input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2a36077-cd72-4c82-9365-58b6759ba78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = params['wte']\n",
    "wpe = params['wpe']\n",
    "x = wte[input_ids] + wpe[range(len(input_ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e7855-bbe5-4e6b-9284-4c12b76c57a2",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "The blocks are a list of repeating transformer blocks `type(params['blocks']) # list` where each block `params['blocks'][0].keys()` consists of ` dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])`.\n",
    "\n",
    "```python\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "#### layer norm\n",
    "\n",
    "The layer_norm weights `params['blocks'][0]['ln_1'].keys()` consist of a   gamma and beta params`dict_keys(['b', 'g'])` which are also called the scale and offset weights because g multiples each element by a factor and be shifts the entire vector `g * x + b` , both  `g` and `b` have the same shape `(768,)`\n",
    "\n",
    "#### multi-layer-perceptron (mlp) aka feed forward net (ffn) \n",
    "\n",
    "This is covered in most basic machine learning classes, so it should suffice that in NumPy, the `@` symbol is used as the matrix multiplication operator, that `ffn` has the same input and output shape and that this is the implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "    return x\n",
    "```\n",
    "\n",
    "Both the layer norm, the ffn and multi headed attention (mha) and the overall transformer block have the same input and output shape\n",
    "\n",
    "#### causal mask\n",
    "\n",
    "```python\n",
    "# causal mask to hide future inputs from being attended to\n",
    "# [n_seq, n_seq]\n",
    "causal_mask = (1 - np.tri(3, dtype=x.dtype)) * -1e10  \n",
    "causal_mask\n",
    "```\n",
    "\n",
    "```\n",
    "array([[-0.e+00, -1.e+10, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -0.e+00]], dtype=float32)\n",
    "```\n",
    "\n",
    "The very negative values cause these positions to have an attention score of nearly 0 after the row-wise softmax is applied.\n",
    "Causing no attention weight to be placed on future tokens\n",
    "\n",
    "```\n",
    "[[ 0, -1000,   -1000],\n",
    " [ 0,     0,   -1000],\n",
    " [ 0,     0,       0]]\n",
    "```\n",
    "#### Attention (Scaled Dot Product QKV attention)\n",
    "\n",
    "kv-caching has to do with improving the efficiency for this attention step. \n",
    "\n",
    "```python\n",
    "# Q, K, V -> A\n",
    "# [n_seq_q, n_embd], [n_seq_k, n_embd], [n_seq_k, n_embd], [n_seq_q, n_seq_k] -> [n_seq_q, n_embd]\n",
    "def attention(q, k, v, mask):  \n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "```\n",
    "\n",
    "#### Multi Headed Attention (mha)\n",
    "\n",
    "multi-headed attention is instead of applying the attention function to Q K V, chopping Q, K V\n",
    "into multiple segments and applying attention between those corresponding segments, then concatenating the result\n",
    "\n",
    "```python\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def mha(x, c_attn, c_proj, n_head):  \n",
    "    \n",
    "    # qkv projection\n",
    "    # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "    x = linear(x, **c_attn)  \n",
    "\n",
    "    # split into qkv\n",
    "    # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "    qkv = np.split(x, 3, axis=-1)  \n",
    "\n",
    "    # split into heads\n",
    "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    # [n_seq, n_seq]\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  \n",
    "\n",
    "    # perform attention over each head\n",
    "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  \n",
    "    \n",
    "    # merge heads\n",
    "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "    x = np.hstack(out_heads)  \n",
    "\n",
    "    # out projection\n",
    "    # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    x = linear(x, **c_proj)  \n",
    "\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a114a6b0-21e4-413a-9b6f-a5feb2b02c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv_proj.shape (10, 2304)\n",
      "len(qkv),(qkv[0].shape) 3 (10, 768)\n",
      " len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape 3 12 (10, 64)\n"
     ]
    }
   ],
   "source": [
    "ln_1 = params['blocks'][0]['ln_1']\n",
    "\n",
    "x_ln = layer_norm(x, **ln_1) # x thanks been layer normed\n",
    "\n",
    "attn = params['blocks'][0]['attn']\n",
    "\n",
    "n_head = hparams['n_head']\n",
    "\n",
    "c_attn = attn['c_attn']\n",
    "\n",
    "c_proj = attn['c_proj']\n",
    "\n",
    "qkv_proj = linear(x_ln, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "print(\"qkv_proj.shape\",qkv_proj.shape) # (10, 2304), 768 x 3 = 2304\n",
    "\n",
    "qkv = np.split(qkv_proj, 3, axis=-1) # [n_seq, 3*n_embd] -> List[3, (n_seq, n_embd)]\n",
    "\n",
    "print(\"len(qkv),(qkv[0].shape)\",len(qkv),(qkv[0].shape)) # list of each head's qkv projection \n",
    "\n",
    "# split into heads\n",
    "# [3, n_seq, n_embd] -> List[3, List[n_head (n_seq, n_embd/n_head)]]\n",
    "qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "print(\"len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape\", len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape)\n",
    "\n",
    "# causal mask to hide future inputs from being attended to\n",
    "causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "# perform attention over each head\n",
    "# [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] \n",
    "\n",
    "# merge heads\n",
    "# [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "x_out = np.hstack(out_heads)  \n",
    "\n",
    "# out projection\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "x_out = linear(x_out, **c_proj)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "be7b5527-dd04-4965-a23c-e46768918721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01042262,  0.22827548, -0.7129095 , -0.9784453 ], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_proj[0,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5fa8574b-ce8d-44cc-9924-50ae61e9da49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01042262,  0.22827548, -0.7129095 , -0.9784453 ], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_heads[0][0][0,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9133fd-6834-43fa-bd3e-6600cb48f5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
