{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ba90b98-bb13-44b4-b609-b8095794f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "    \n",
    "from pico.utils import load_encoder_hparams_and_params\n",
    "from pico.gpt2 import generate, generate_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641f966-b125-4e95-bf09-3b21b940f766",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "1. https://www.dipkumar.dev/posts/gpt-kvcache/\n",
    "2. https://github.com/jaymody/picoGPT/pull/7/files\n",
    "\n",
    "You can also control the number of tokens to generate, the model size (one of `[\"124M\", \"355M\", \"774M\", \"1558M\"]`), and the directory to save the models:\n",
    "\n",
    "expected_completion = ' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24853542-495d-4ab9-b6c9-76d7575ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kb [00:00, 2.96Mb/s]                                                       \n",
      "Fetching encoder.json: 1.04Mb [00:00, 2.61Mb/s]                                                     \n",
      "Fetching hparams.json: 1.00kb [00:00, 3.19Mb/s]                                                     \n",
      "Fetching model.ckpt.data-00000-of-00001: 3.10Gb [04:55, 10.5Mb/s]                                   \n",
      "Fetching model.ckpt.index: 16.0kb [00:00, 9.58Mb/s]                                                 \n",
      "Fetching model.ckpt.meta: 1.38Mb [00:00, 3.38Mb/s]                                                  \n",
      "Fetching vocab.bpe: 457kb [00:00, 1.66Mb/s]                                                         \n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_generate = 40\n",
    "tokenizer, hparams, params = load_encoder_hparams_and_params(\n",
    "    model_size = \"774M\", #\"124M\", \n",
    "    models_dir = \"models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5841c5a2-2733-4d25-9dd1-33c41edb3800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:18<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 18.19883704185486 seconds to complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' so powerful that they could be used to solve problems that humans could not.\\n\\nIn the 1950s, Turing was asked to help develop a computer program that could play chess. He was given a'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "# decode the ids back into a string\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The process took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e94a10e9-d82e-443a-baf7-2ed25f0ac488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:49<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 49.79473090171814 seconds to complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' so powerful that they could be used to solve problems that humans could not.\\n\\nIn the 1950s, Turing was asked to help develop a computer program that could play chess. He was given a'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate_kv(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "# decode the ids back into a string\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The process took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d7979-9cba-4ad0-bf1e-db7e6ea45040",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "Before we learn kv-cache, lets first understand the non-kv-cache version of the autoregressive generation.\n",
    "\n",
    "First, a tokenizer converts our text into a list of token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251ca01a-7357-4c5b-808f-f8748955f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input string using the BytePairEncoding tokenizer\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "print(len(input_ids))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc3434-1abd-46b4-9f60-4d992c4f7303",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "The weights to this LLM are the\n",
    "\n",
    "1. Word Positional Encoding (wpe)\n",
    "2. Word Token Embeddings (wte)\n",
    "\n",
    "```python\n",
    "print(params.keys()) # dict_keys(['blocks', 'ln_f', 'wpe', 'wte'])\n",
    "```\n",
    "\n",
    "By passing `**params` into `logits = gpt2(inputs, **params, n_head=n_head)` we are just passing this dictionary's values, the weights akak parameters, into the function as arguments using the dictionary keys as the names `wte, wpe, blocks, ln_f`\n",
    "\n",
    "#### Word Positional Encoding\n",
    "\n",
    "The Word Positional Encoding (wpe) is used to add a vector that represents a position in time, or order in a sequence, to each token embedding. its `print(type(params['wpe']), params['wpe'].shape)` is `<class 'numpy.ndarray'> (1024, 768)` because we have precalculated for you the first 1024 of these positional embeddings, and our embedding size is 768. In doing `wpe[range(len(inputs))]` we have just selected the first `len(inputs)` embeddings\n",
    "\n",
    "#### Word Token Embedding\n",
    "\n",
    "The Word Token Embedding is used to map each token_id (input_ids) to its corresponding vector. `print(type(params['wte']), params['wte'].shape)` is `<class 'numpy.ndarray'> (50257, 768)` because our vocab size is 50257 and our embedding size is 768. In doing `wte[inputs]` we have just mapped our token id list of size 10 to a sequence of embeddings shape (10, 768)\n",
    "\n",
    "#### Transformer Input Embeddings\n",
    "\n",
    " the embedings that go into the first of multiple transformer blocks is the element-wise sum of wte and wpe `x = wte[inputs] + wpe[range(len(inputs))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff499b14-d0bb-4420-9270-23d21fe42b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.88207198e-02 -1.97418600e-01  4.02672496e-03 ... -4.30437364e-02\n",
      "   2.82671917e-02  5.44901080e-02]\n",
      " [ 2.39594337e-02 -5.37920333e-02 -9.48786438e-02 ...  3.41700129e-02\n",
      "   1.01718502e-02 -1.55729489e-04]\n",
      " [ 4.21607168e-03 -8.47639143e-02  5.45149297e-02 ...  1.97447110e-02\n",
      "   1.93248559e-02 -2.14238558e-02]\n",
      " ...\n",
      " [ 2.53077131e-03 -3.17870919e-03  1.17414258e-01 ...  2.00962462e-03\n",
      "   4.41795774e-03 -6.83258474e-03]\n",
      " [-1.23805739e-03 -1.77337788e-03  1.11044556e-01 ... -2.30074697e-03\n",
      "   4.15364839e-03 -1.04475096e-02]\n",
      " [ 4.93714586e-03  2.14576256e-03  1.17781341e-01 ... -2.82027118e-04\n",
      "   4.07085707e-03 -5.54985739e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wpe'][range(len(input_ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "882d9fd3-ac2c-4b74-8340-9e6828486d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04486499 -0.1522257   0.10908855 ...  0.16187134  0.00406003\n",
      "  -0.01259668]\n",
      " [-0.1435177  -0.1303647  -0.00709237 ... -0.26905674 -0.21710931\n",
      "  -0.27703205]\n",
      " [-0.14161602 -0.06058507  0.05428597 ...  0.16568261  0.1750053\n",
      "   0.08499283]\n",
      " ...\n",
      " [ 0.00818344  0.03351058  0.03436588 ...  0.15731247  0.06635052\n",
      "  -0.08678364]\n",
      " [-0.1378994  -0.02936367 -0.00255402 ... -0.09662744 -0.07259481\n",
      "   0.11599892]\n",
      " [ 0.06102467 -0.072351    0.01882253 ... -0.24272189  0.23248099\n",
      "   0.12684126]]\n"
     ]
    }
   ],
   "source": [
    "print(params['wte'][input_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2a36077-cd72-4c82-9365-58b6759ba78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = params['wte']\n",
    "wpe = params['wpe']\n",
    "x = wte[input_ids] + wpe[range(len(input_ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e7855-bbe5-4e6b-9284-4c12b76c57a2",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "The blocks are a list of repeating transformer blocks `type(params['blocks']) # list` where each block `params['blocks'][0].keys()` consists of ` dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])`.\n",
    "\n",
    "```python\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    \n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "#### layer norm\n",
    "\n",
    "The layer_norm weights `params['blocks'][0]['ln_1'].keys()` consist of a gamma and beta params`dict_keys(['b', 'g'])` which are also called the scale and offset weights because g multiples each element by a factor and be shifts the entire vector `g * x + b` , both  `g` and `b` have the same shape `(768,)`\n",
    "\n",
    "#### multi-layer-perceptron (mlp) aka feed forward net (ffn) \n",
    "\n",
    "This is covered in most basic machine learning classes, so it should suffice that in NumPy, the `@` symbol is used as the matrix multiplication operator, that `ffn` has the same input and output shape and that this is the implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "    return x\n",
    "```\n",
    "\n",
    "Both the layer norm, the ffn and multi headed attention (mha) and the overall transformer block have the same input and output shape\n",
    "\n",
    "#### causal mask\n",
    "\n",
    "```python\n",
    "# causal mask to hide future inputs from being attended to\n",
    "# [n_seq, n_seq]\n",
    "causal_mask = (1 - np.tri(3, dtype=x.dtype)) * -1e10  \n",
    "causal_mask\n",
    "```\n",
    "\n",
    "```\n",
    "array([[-0.e+00, -1.e+10, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -1.e+10],\n",
    "       [-0.e+00, -0.e+00, -0.e+00]], dtype=float32)\n",
    "```\n",
    "\n",
    "The very negative values cause these positions to have an attention score of nearly 0 after the row-wise softmax is applied.\n",
    "Causing no attention weight to be placed on future tokens\n",
    "\n",
    "```\n",
    "[[ 0, -1000,   -1000],\n",
    " [ 0,     0,   -1000],\n",
    " [ 0,     0,       0]]\n",
    "```\n",
    "#### Attention (Scaled Dot Product QKV attention)\n",
    "\n",
    "Here is a moving diagram of Scaled Dot Product QKV attention as a tranformer starts from 1 token and each of the next 3 tokens it generates attends over the previous positions. The grey squares represent the causal mask and though not shown in the diagram, a softmax is applied to `QK^T` before it is matrix multiplied with V to produce A. Watch the diagram evolve, notice that at each step, the upper left square of the `QK^T` matrix is recomputed at every step. Only the bottom row and right column are new. Not only that, notice that because of causal masking, only the bottom row is both new and also need, for matrix multiplication with V to find the next A vector. kv-caching has to do with improving the efficiency for this attention step. \n",
    "\n",
    "<img src=\"samples/QKV_scaled_dot_prod_attn.gif\" height = 500 width = 1000 >\n",
    "\n",
    "```python\n",
    "# Q, K, V -> A\n",
    "def attention(Q, K, V, mask): \n",
    "    \n",
    "    # [n_seq_q, n_embd], [n_seq_k, n_embd], [n_seq_k, n_embd], [n_seq_q, n_seq_k] -> [n_seq_q, n_embd]\n",
    "    \n",
    "    QK_T = Q @ K.T\n",
    "    \n",
    "    A = softmax(QK_T / np.sqrt(Q.shape[-1]) + mask) @ V\n",
    "    \n",
    "    return A\n",
    "```\n",
    "\n",
    "#### Multi Headed Attention (mha)\n",
    "\n",
    "multi-headed attention is instead of applying the attention function to Q K V, chopping Q, K V\n",
    "into multiple segments and applying attention between those corresponding segments, then concatenating the result\n",
    "\n",
    "```python\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def mha(x, c_attn, c_proj, n_head):  \n",
    "    \n",
    "    # qkv projection\n",
    "    # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "    x = linear(x, **c_attn)  \n",
    "\n",
    "    # split into qkv\n",
    "    # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "    qkv = np.split(x, 3, axis=-1)  \n",
    "\n",
    "    # split into heads\n",
    "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    # [n_seq, n_seq]\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  \n",
    "\n",
    "    # perform attention over each head\n",
    "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  \n",
    "    \n",
    "    # merge heads\n",
    "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "    x = np.hstack(out_heads)  \n",
    "\n",
    "    # out projection\n",
    "    # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    x = linear(x, **c_proj)  \n",
    "\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a114a6b0-21e4-413a-9b6f-a5feb2b02c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv_proj.shape (10, 2304)\n",
      "[ 0.01042262  0.22827548 -0.7129095  -0.9784453 ]\n",
      "len(qkv),(qkv[0].shape) 3 (10, 768)\n",
      "len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape 3 12 (10, 64)\n",
      "[ 0.01042262  0.22827548 -0.7129095  -0.9784453 ]\n",
      "len(out_heads), out_heads[0].shape 12 (10, 64)\n",
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "# code run thru of multi-headed attention\n",
    "\n",
    "ln_1 = params['blocks'][0]['ln_1']\n",
    "attn = params['blocks'][0]['attn']\n",
    "n_head = hparams['n_head']\n",
    "c_attn = attn['c_attn']\n",
    "c_proj = attn['c_proj']\n",
    "\n",
    "x_ln = layer_norm(x, **ln_1) # x thanks been layer normed\n",
    "\n",
    "qkv_proj = linear(x_ln, **c_attn) # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "print(\"qkv_proj.shape\",qkv_proj.shape) # (10, 2304), 768 x 3 = 2304\n",
    "print(qkv_proj[0,:4])\n",
    "\n",
    "qkv = np.split(qkv_proj, 3, axis=-1) # [n_seq, 3*n_embd] -> List[3, (n_seq, n_embd)]\n",
    "print(\"len(qkv),(qkv[0].shape)\",len(qkv),(qkv[0].shape)) # list of each head's qkv projection \n",
    "\n",
    "# split into heads\n",
    "# [3, n_seq, n_embd] -> List[3, List[n_head (n_seq, n_embd/n_head)]]\n",
    "qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "print(\"len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape\", len(qkv_heads), len(qkv_heads[0]), qkv_heads[0][0].shape)\n",
    "print(qkv_heads[0][0][0,:4])\n",
    "\n",
    "# causal mask to hide future inputs from being attended to\n",
    "causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "# perform attention over each head\n",
    "# List[3, List[n_head (n_seq, n_embd/n_head)]]] -> List[n_head, (n_seq, n_embd/n_head)]\n",
    "out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] \n",
    "\n",
    "print(\"len(out_heads), out_heads[0].shape\", len(out_heads), out_heads[0].shape)\n",
    "\n",
    "# merge heads\n",
    "# List[n_head, (n_seq, n_embd/n_head)] -> [n_seq, n_embd]\n",
    "x_out = np.hstack(out_heads)  # stack horizontally, meaning preserve the\n",
    "\n",
    "# out projection\n",
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "x_out = linear(x_out, **c_proj)  \n",
    "print(x_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7b15c-a7d7-4af2-b75a-73d4cf3e3225",
   "metadata": {},
   "source": [
    "# KV Caching\n",
    "\n",
    "The moving diagram below compares non-kv-caching on top with kv-caching on the bottom row as the transformer starts with 1 token and generate 3 more autoregressively step wise. With each new step, we only calculate the QKV projection for the most recent token. As we discussed previously, only the bottom row of the `QK^T` attention matrix is used at each step. To compute the next bottom row, you dont actually need the previous Q projections, so thats why this isnt called a QKV cache. You do still need all the previous K projections and V projections, also `QK^T` is no longer a square attention matrix but rather a new sequence_length sized vector at each step that represents the newest V projection's attention on all previous V projections.\n",
    "\n",
    "<img src=\"samples/KV_cache.gif\">\n",
    "\n",
    "The benefit is that instead of doing a (n_seq x emb_dim) x (emb_dim x n_seq) -> O(n_seq^2 x emb_dim) of compute, you now are doing\n",
    "(emb_dim) x (emb_dim x n_seq) -> O(n_seq x emb_dim) of compute. The tradeoff is that we now need to keep a growing Key and Value states in GPU VRAM or CPU RAM. Lastly notice that we dont have to change the scaled dot product QKV `attention()` function, we just need to pass a new shape Q and mask, both vectors instead of matrices, into the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fa8574b-ce8d-44cc-9924-50ae61e9da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "def mha(x, c_attn, c_proj, n_head, kv_cache=None):  \n",
    "\n",
    "    \"\"\" with the KV cache strategy, we will only be passing the last token\n",
    "    into mha, so n_seq = 1 in the qkv projection and split steps\n",
    "    \"\"\"\n",
    "    \n",
    "    # qkv projection\n",
    "    # [n_seq = 1, n_embd] -> [n_seq = 1, 3*n_embd]\n",
    "    x = linear(x, **c_attn)  \n",
    "\n",
    "    # split into qkv\n",
    "    # [n_seq = 1, 3*n_embd] -> [3, n_seq = 1, n_embd]\n",
    "    qkv = np.split(x, 3, axis=-1)  \n",
    "\n",
    "    if kv_cache:\n",
    "\n",
    "        # these are all vectors\n",
    "        new_q, new_k, new_v = qkv  # new_q, new_k, new_v = [1, n_embd]\n",
    "        \n",
    "        # append new_k and new_v to the old_k and old_v before multiplying with new_q\n",
    "        old_k, old_v = kv_cache\n",
    "        k = np.vstack([old_k, new_k]) # k shaped (n_seq, n_embd), where n_seq = prev_n_seq + 1\n",
    "        v = np.vstack([old_v, new_v]) # v shaped (n_seq, n_embd), where n_seq = prev_n_seq + 1\n",
    "        qkv = [new_q, k, v] # new_q is a vector, k and v are matrices\n",
    "\n",
    "        # if kvcache, we passing a single token as input which need to attend to all previous tokens\n",
    "        # so we create vector shaped empty mask with all 0s the shape of n_seq\n",
    "        causal_mask = np.zeros((1, k.shape[0]))\n",
    "        \n",
    "    else:\n",
    "        # create triangular causal mask to hide future inputs from being attended to\n",
    "        causal_mask = (1 - np.tri(x.shape[0])) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    current_cache = [qkv[1], qkv[2]] # store k and v in the cache\n",
    "\n",
    "    # split into heads\n",
    "    # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  \n",
    "\n",
    "    # perform attention over each head\n",
    "    # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  \n",
    "    \n",
    "    # merge heads\n",
    "    # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "    x = np.hstack(out_heads)  \n",
    "\n",
    "    # out projection\n",
    "    # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    x = linear(x, **c_proj)  \n",
    "\n",
    "    # we pass the updated_cache along to the next timestep\n",
    "    return x, current_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce9133fd-6834-43fa-bd3e-6600cb48f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head, kv_cache=None):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # multi-head causal self attention\n",
    "    # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    attn_out, kv_cache_updated = mha(layer_norm(x, **ln_1), **attn, n_head=n_head, kv_cache=kv_cache)\n",
    "    x = x + attn_out  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x, kv_cache_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c893bf-07e3-462b-8036-ed680612a9af",
   "metadata": {},
   "source": [
    "### projection to vocab space\n",
    "\n",
    "it makes alot of sense to use `x @ wte.T` to project your sequence of embeddings back into vocab space because your logits will be proportional to the dot product between transformer output x and the token embedding wte. For example if x was a sequence of embeddings most similar to the mebeddings for \"this is a cat\", in word embeddings, then you would expect the largest logits to be in the token indices for the word \"this\", \"is\", \"a\" and \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70bfa305-be5b-4d3f-8b2a-40d018328f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head, kv_cache = None):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    \n",
    "    if not kv_cache:\n",
    "        kv_cache = [None]*len(blocks)\n",
    "        wpe_out = wpe[range(len(inputs))]\n",
    "    else:\n",
    "        wpe_out = wpe[[len(inputs)-1]]\n",
    "        inputs = [inputs[-1]]\n",
    "\n",
    "    x = wte[inputs] + wpe_out  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    layerwise_kv_cache = []\n",
    "    for block, kv_cache_block in zip(blocks, kv_cache):\n",
    "        x, kv_cache_updated = transformer_block(x, **block, n_head=n_head, kv_cache=kv_cache_block)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "        # TODO: inplace extend new cache instead of re-saving whole layerwise_kv_cache from kv_cache\n",
    "        layerwise_kv_cache.append(kv_cache_updated)  \n",
    "\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    logits = x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
    "    \n",
    "    return logits, layerwise_kv_cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71c8b9fa-90ab-474e-bfff-65eda75165f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "\n",
    "    kvcache = None\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
    "        logits, kvcache = gpt2(inputs, **params, n_head=n_head, kv_cache=kvcache)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0b2a4af-ca94-41c3-9b6d-fbde7ca445d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "# make sure we are not surpassing the max sequence length of our model\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "expected_completion = ' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b27fd71d-e58b-4643-a1d5-98b2cc2b3e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:12<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process took 12.223795890808105 seconds to complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' the most powerful machines on the planet.\\n\\nThe computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "# decode the ids back into a string\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"The process took {elapsed_time} seconds to complete.\")\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e60dd-cbb0-4f4b-9402-57d396ae2c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
